# -*- coding: utf-8 -*-
"""Car_Price_28_March_PIAIC_91502.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WrnrewdDm2_0YFIO96x5y91MakBO-tiU

# Car Price Prediction::

Download dataset from this link:

https://www.kaggle.com/hellbuoy/car-price-prediction

# Problem Statement::

A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.

They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:

Which variables are significant in predicting the price of a car
How well those variables describe the price of a car
Based on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.

# task::
We are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.

# WORKFLOW ::

1.Load Data

2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )

3.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).

4.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).

5.Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)
6.Train the Model with Epochs (100) and validate it

7.If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .

8.Evaluation Step

9.Prediction

# Step 1
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense 
from tensorflow.keras.optimizers import Adam, SGD

"""# LOAD DATA"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/PIAIC/AI_assignments/CarPrice_Assignment.csv')

data.head()

data.shape

data.CarName.unique()

data.CarName.value_counts()  #Return a Series containing counts of unique values

print(data.columns)

"""# Checking missing Values"""

data.isnull()

data.isnull().sum()

data.info()

data.shape

"""## convert non-numerical values into numerical values::

"""

from sklearn.preprocessing import LabelEncoder

"""Initialize LabelEncoder"""

labelencoder = LabelEncoder() #Encode target labels with value between 0 and n_classes-1

data['fueltype'] = labelencoder.fit_transform(data['fueltype'])
data['aspiration'] = labelencoder.fit_transform(data['aspiration'])
data['carbody'] = labelencoder.fit_transform(data['carbody'])
data['drivewheel'] = labelencoder.fit_transform(data['drivewheel'])
data['enginelocation'] = labelencoder.fit_transform(data['enginelocation'])
data['fuelsystem'] = labelencoder.fit_transform(data['enginelocation'])

data['doornumber'] = data['doornumber'].map({'two':2,'four':4})

data.head(10)

"""The .corr() method will actually get rid of the columns that are not suited for correlation for us if we wanted less categories in our heatmap, we could select only those, similar to the eample shown """

corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""With the help of above figure we identify our relevent correalted features. Now it is easy for us to focus on those features which are necessary for us."""

data1 = data[['horsepower','boreratio','enginesize','curbweight','carwidth','carlength',
              'wheelbase','drivewheel','price','enginelocation']]      # List of our final features

print(data1.shape)
data1.head()

x = (data1.loc[:, data1.columns != 'price'])   # loc[row , column]      data1.loc[all rows, price column excluded]
y = (data1.loc[:, data1.columns == 'price'])

x

y

"""**#Split data**

Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).
"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)

x_train.shape  # (143/205) * 100 = 69.75 % our training part is 70 %

x_test

x_train

y_train.shape

y_test.shape   # (62/205) * 100 = 30.24 % our test part is 30 %

x_train

"""Normalize the data

we normalize data because data has big vlaues in decimal and it will worsen performance of our model, may overfit 
 or  we may face hardware resource high usage
 we will apply the formula normalized_train_data = (train_data - mean)/ stadrad_deviation
 firt take mean of training, then subtract mean from each value of the array
"""

mean = x_train.mean(axis=0)   # Normalize the data part
x_train -= mean                # Keep in mind the mean will be of traing part     

std = x_train.std(axis=0)

x_train /= std

x_test -= mean
x_test /= std

mean_label = y_train.mean(axis=0)   # Normalize the data labels   ....(price)
y_train -= mean_label                # Keep in mind the mean will be of traing part     

std_label = y_train.std(axis=0)

y_train /= std_label

y_test -= mean_label
y_test /= std_label

x_train

y_train

x_test

y_test



"""# **Standardized the data**"""

#x_train, x_test, y_train, y_test
type(x_train)

x_train = x_train.to_numpy()
y_train = y_train.to_numpy().astype('float32')

x_test = x_test.to_numpy()
y_test = y_test.to_numpy().astype('float32')

"""#Create Model"""

# we are passing activation function as a parameter here so that we can call this function with tanh or relu while
# fitting and training the model
from keras import models
from keras import layers

def build_model(act):
  model = models.Sequential()
  model.add(layers.Dense(10, activation= act,input_shape=(9,)))
  model.add(layers.Dense(8, activation= act))
  model.add(layers.Dense(6, activation= act))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model

build_model('relu').summary()

build_model('tanh').summary()

# Regularized model
from keras import regularizers
def build_model_regular(act):
  model = models.Sequential()
  model.add(layers.Dense(10, activation= act,kernel_regularizer= regularizers.l1_l2(l1=0.001, l2=0.001),input_shape=(9,)))
  model.add(layers.Dense(8, activation= act,kernel_regularizer= regularizers.l1_l2(l1=0.001, l2=0.001)))
  model.add(layers.Dense(6, activation= act,kernel_regularizer= regularizers.l1_l2(l1=0.001, l2=0.001)))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model

build_model_regular('relu').summary()

build_model_regular('tanh').summary()

# dropout model
from keras import regularizers
def build_model_drop(act):
  model = models.Sequential()
  model.add(layers.Dense(10, activation= act,input_shape=(9,)))
  model.add(layers.Dropout(0.1))
  model.add(layers.Dense(8, activation= act))
  model.add(layers.Dropout(0.1))
  model.add(layers.Dense(6, activation= act))
  model.add(layers.Dropout(0.1))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model

"""# K fold Validation"""

#k fold validation with relu
# 143/4
import numpy as np
k =  4
num_val_samples = len(x_train) // k    # x_train, x_test, y_train, y_test
num_epochs = 100
all_scores_relu = []
for i in range(k):
  print('processing fold #', i)
  val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]
  partial_train_data = np.concatenate([x_train[:i * num_val_samples],x_train[(i + 1) * num_val_samples:]],  axis=0)
  # print(partial_train_data)
  partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)
  model = build_model('relu')
  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores_relu.append(val_mae)

#k fold validation with tanh
# 143/4
import numpy as np
k =  4
num_val_samples = len(x_train) // k
num_epochs = 100
all_scores_tanh = []
for i in range(k):
  print('processing fold #', i)
  val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]
  partial_train_data = np.concatenate([x_train[:i * num_val_samples],x_train[(i + 1) * num_val_samples:]],  axis=0)
  # print(partial_train_data)
  partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)
  model = build_model('tanh')
  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores_tanh.append(val_mae)

#k fold validation with regularization
# 143/4
import numpy as np
k =  4
num_val_samples = len(x_train) // k
num_epochs = 100
all_scores_regular = []
for i in range(k):
  print('processing fold #', i)
  val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]
  partial_train_data = np.concatenate([x_train[:i * num_val_samples],x_train[(i + 1) * num_val_samples:]],  axis=0)
  # print(partial_train_data)
  partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)
  model = build_model_regular('relu')
  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores_regular.append(val_mae)

#k fold validation with drop out
# 143/4
import numpy as np
k =  4
num_val_samples = len(x_train) // k
num_epochs = 100
all_scores_drop = []
for i in range(k):
  print('processing fold #', i)
  val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]
  partial_train_data = np.concatenate([x_train[:i * num_val_samples],x_train[(i + 1) * num_val_samples:]],  axis=0)
  # print(partial_train_data)
  partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)
  model = build_model_drop('relu')
  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores_drop.append(val_mae)

"""# Scores of all models during training"""

all_scores_relu

all_scores_tanh

all_scores_regular

all_scores_drop

"""#Now we will train our models on training data and then evaluate"""

model_tanh = build_model('tanh')
model_tanh.fit(x_train, y_train,epochs= 80, batch_size=1, verbose=0)
test_mse_score, test_mae_score = model_tanh.evaluate(x_test, y_test)

model_regular = build_model_regular('relu')
model_regular.fit(x_train, y_train,epochs= 80, batch_size=1, verbose=0)
test_mse_score, test_mae_score = model_regular.evaluate(x_test, y_test)

model_drop = build_model_drop('relu')
model_drop.fit(x_train, y_train,epochs= 80, batch_size=1, verbose=0)
test_mse_score, test_mae_score = model_drop.evaluate(x_test, y_test)

"""# Predict

here we will predict our prices of our test dataset with each model which we have trained in training section
Note that here we will use the reverse process of Normalization to retrieve our values of price in thousand of dollars i.e. x = (y - mean)/ std ==>> we will calculate( y = x * std + mean) and then we will compare it with our target values
"""

len(model_tanh.predict(x_test))

model_tanh.predict(x_test)

model_regular.predict(x_test)

model_drop.predict(x_test)