# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection assignment_25_March_PIAIC_91502.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1saIIX7VA1rh8E-X6pouBQzejeHmHmmlN

# Credit Card Fraud Detection::

Download dataset from this link:

https://www.kaggle.com/mlg-ulb/creditcardfraud

# Description about dataset::

The datasets contains transactions made by credit cards in September 2013 by european cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. 


### Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

# WORKFLOW :

1.Load Data

2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )

3.Standardized the Input Variables. 

4.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).

5.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).

6.Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)

7.Train the Model with Epochs (100).

8.If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .

9.Prediction should be > 92%
10.Evaluation Step
11Prediction

# Task::

## Identify fraudulent credit card transactions.

# Step 1

Import all the libraries which we need
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense 
from tensorflow.keras.optimizers import Adam, SGD

from tensorflow.keras import models
from tensorflow.keras import layers

"""**Mount Google Drive**"""

from google.colab import drive
drive.mount('/content/drive')

"""**Load the Data**"""

creditcard_data = pd.read_csv('/content/drive/MyDrive/PIAIC/AI_assignments/creditcard.csv')

creditcard_data.shape

creditcard_data.head()

creditcard_data['Class'].head()

creditcard_data['Class'].shape

#fraud = creditcard_data[creditcard_data['Class']==1]
#normal = creditcard_data[creditcard_data['Class']==0]

#fraud.shape

#normal.shape

#fraud.shape[0]

count_classes = pd.value_counts(creditcard_data['Class'], sort = True)

count_classes

count_classes.plot(kind= 'bar')
plt.xticks(range(2), labels= ['normal', 'fraud'])



fraud.shape

normal.shape

fraud.head()

normal.head()



creditcard_data.info()

"""**Check Missing values**"""

creditcard_data.isnull().values.any()

creditcard_data.isnull().sum()

creditcard_data.isnull().any()

creditcard_data.shape

creditcard_data.values

creditcard_data.value_counts()

"""Seperate the Data from its Labels

Seperate the "Feature" class from other features
"""

#  Variable "x" is our data
#  Variable "y" is our label
x = (creditcard_data.loc[:, creditcard_data.columns != 'Class'])   # loc[row , column]      data1.loc[all rows, price column excluded]
y = (creditcard_data.loc[:, creditcard_data.columns == 'Class'])

"""**Standardized the input variables**"""

# Get the Fraud and normal data set
fraud  = creditcard_data[creditcard_data['Class']== 1]
normal = creditcard_data[creditcard_data['Class']==0]

normal = normal.sample(2*fraud.shape[0])
data = fraud.append(normal, ignore_index=True)
x_data = data.drop(columns = 'Class', axis = 0)
label = data['Class']

"""**Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).**"""

from sklearn.model_selection import train_test_split
train_data, test_data, train_labels, test_labels = train_test_split(x_data, label, test_size=0.30, random_state=42)

train_data

train_labels

fraud.sample()

test_data

test_labels

train_labels.dtype

test_labels.dtype

label.values

x_data.value_counts

x_data.values

"""**Normalize the data**"""

mean=train_data.mean(axis=0)
train_data -= mean
std=train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data -=std

train_data

test_data

"""# **Standardized our data..........**"""

train_data = train_data.to_numpy()
test_data = test_data.to_numpy()

train_labels=np.asarray(train_labels).astype(dtype="float64")
test_labels=np.asarray(test_labels).astype(dtype="float64")

train_labels

test_labels

"""**Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).**"""

train_data.shape

train_data.shape[1]

"""**Model**"""

model = models.Sequential()
  model.add(layers.Dense(10, activation= 'relu',input_shape=(train_data.shape[1],)))
  model.add(layers.Dense(8, activation= 'relu'))
  model.add(layers.Dense(6, activation= 'relu'))
  model.add(layers.Dense(1, activation="sigmoid"))

"""**Compile**"""

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])



"""**Train the Model with Epochs (100)**"""

history = model.fit(train_data, train_labels, epochs=100, validation_split=.2)   # Split into 50% Training(Samples,Labels) and 20% Validation Data(Samples,Labels).

loss=history.history["loss"]
val_loss=history.history["val_loss"]
epochs=range(1, len(loss)+1)
plt.plot(epochs, loss, "bo", label="Training Loss")
plt.plot(epochs, val_loss, "b", label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

acc=history.history["accuracy"]
val_acc=history.history["val_accuracy"]
epochs=range(1, len(acc)+1)
plt.plot(epochs[:], acc[:], "bo", label="Training Accuracy")
plt.plot(epochs[:], val_acc[:], "b", label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""**Evaluate**"""

test_loss_score, test_acc_score=model.evaluate(test_data, test_labels)

"""**Predict**"""

prediction=model.predict(test_data).astype(dtype="u8")

prediction