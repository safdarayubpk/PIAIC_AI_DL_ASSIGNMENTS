# -*- coding: utf-8 -*-
"""Concrete Strength Assignment_25_March_PIAIC_91502.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16uiMHiJA-gUu6Fm8EtQsE7OZPhfNs2ph

# Assignment: Compresive Strength Concrete Problem


### Abstract: 

Concrete is the most important material in civil engineering. The concrete compressive strength (concrete strength to bear the load) is a highly nonlinear function of age and ingredients.  <br><br>

<table border="1"  cellpadding="6" bordercolor="red">
	<tbody>
        <tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Data Set Characteristics:&nbsp;&nbsp;</b></p></td>
		<td><p class="normal">Multivariate</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Instances:</b></p></td>
		<td><p class="normal">1030</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Area:</b></p></td>
		<td><p class="normal">Physical</p></td>
        </tr>
     </tbody>
    </table>
<table border="1" cellpadding="6">
    <tbody>
        <tr>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Attribute Characteristics:</b></p></td>
            <td><p class="normal">Real</p></td>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Number of Attributes:</b></p></td>
            <td><p class="normal">9</p></td>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Date Donated</b></p></td>
            <td><p class="normal">2007-08-03</p></td>
        </tr>
     </tbody>
    </table>
<table border="1" cellpadding="6">	
    <tbody>
    <tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Associated Tasks:</b></p></td>
		<td><p class="normal">Regression</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Missing Values?</b></p></td>
		<td><p class="normal">N/A</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Web Hits:</b></p></td>
		<td><p class="normal">231464</p></td>
	</tr>
    </tbody>
    </table>

###  Description:
| Features Name | Data Type | Measurement | Description |
| -- | -- | -- | -- |
Cement (component 1) | quantitative | kg in a m3 mixture | Input Variable
Blast Furnace Slag (component 2) | quantitative | kg in a m3 mixture | Input Variable
Fly Ash (component 3) | quantitative | kg in a m3 mixture | Input Variable
Water (component 4) | quantitative | kg in a m3 mixture | Input Variable
Superplasticizer (component 5) | quantitative | kg in a m3 mixture | Input Variable
Coarse Aggregate (component 6) | quantitative | kg in a m3 mixture | Input Variable
Fine Aggregate (component 7) | quantitative | kg in a m3 mixture | Input Variable
Age | quantitative | Day (1~365) | Input Variable
Concrete compressive strength | quantitative | MPa | Output Variable

### WORKFLOW :
- Load Data
- Check Missing Values ( If Exist ; Fill each record with mean of its feature )
- Standardized the Input Variables. **Hint**: Centeralized the data
- Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).
- Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).
- Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)
- Train the Model with Epochs (100) and validate it
- If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .
- Evaluation Step
- Prediction

# Load Data:
[Click Here to Download DataSet](https://github.com/ramsha275/ML_Datasets/blob/main/compresive_strength_concrete.csv)
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense 
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import models
from tensorflow.keras import layers

from google.colab import drive
drive.mount('/content/drive')

compressive_strength_concrete_data = pd.read_csv('/content/drive/MyDrive/PIAIC/AI_assignments/compresive_strength_concrete.csv')

compressive_strength_concrete_data.head()

compressive_strength_concrete_data.shape

compressive_strength_concrete_data.isnull().sum()

print(compressive_strength_concrete_data.columns)

"""**Rename the column names**"""

compressive_strength_concrete_data = compressive_strength_concrete_data.rename(columns= {'Cement (component 1)(kg in a m^3 mixture)' : 'cement',
       'Blast Furnace Slag (component 2)(kg in a m^3 mixture)':'slag',
       'Fly Ash (component 3)(kg in a m^3 mixture)': 'ash',
       'Water  (component 4)(kg in a m^3 mixture)': 'water',
       'Superplasticizer (component 5)(kg in a m^3 mixture)': 'superplastic',
       'Coarse Aggregate  (component 6)(kg in a m^3 mixture)': 'coarseagg',
       'Fine Aggregate (component 7)(kg in a m^3 mixture)': 'fineagg', 'Age (day)': 'age',
       'Concrete compressive strength(MPa, megapascals) ': 'strength'})

compressive_strength_concrete_data.columns

compressive_strength_concrete_data.head()

"""#**Checking Missing values**

> Indented block


"""

compressive_strength_concrete_data.isnull().sum()

compressive_strength_concrete_data.info()

compressive_strength_concrete_data.describe()

compressive_strength_concrete_data.describe().T

"""#**Standardized the Input Variables. **Hint**: Centeralized the data**

---

---

Seperate our target "dependant variable" which is "**strength**" from other 


independant variables
"""

x = compressive_strength_concrete_data.drop('strength', axis=1)
y = compressive_strength_concrete_data['strength']

x.ndim

y.ndim

x.shape

y.shape

x

y

"""Split arrays or matrices into random train and test subsets



"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test =train_test_split(x,y, test_size=0.3, random_state=42)

mean = x_train.mean(axis=0)   # Normalize the data part
x_train -= mean                # Keep in mind the mean will be of traing part     

std = x_train.std(axis=0)

x_train /= std

x_test -= mean
x_test /= std

mean_label = y_train.mean(axis=0)   # Normalize the data labels   ....(strength)
y_train -= mean_label                # Keep in mind the mean will be of traing part     

std_label = y_train.std(axis=0)

y_train /= std_label

y_test -= mean_label
y_test /= std_label

x_train.describe().T

"""#**Standardized the Input Variables. **Hint**: Centeralized the data**

We standardized our input variables which are  "'cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',
       'fineagg', 'age'"
"""

x_train = pd.DataFrame(x_train, columns= ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',
       'fineagg', 'age'])
x_test = pd.DataFrame(x_test, columns= ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',
       'fineagg', 'age'])

x_train.shape

"""# Standardized the Data"""

x_train = x_train.to_numpy()
y_train = y_train.to_numpy().astype('float32')

x_test = x_test.to_numpy()
y_test = y_test.to_numpy().astype('float32')

type(x_train)

type(y_train)

"""# **Build a fuction for model**

Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).

Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)
"""

def build_model():
    model = models.Sequential()
    model.add(layers.Dense(10, activation= 'relu',input_shape=(x_train.shape[1],)))
    model.add(layers.Dense(8, activation= 'relu'))
    model.add(layers.Dense(6, activation= 'relu'))
    model.add(layers.Dense(1))
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    return model

"""# **K-fold Validation**"""

k =  4
num_val_samples = len(x_train) // k   # x_train, x_test, y_train, y_test
num_epochs = 100
all_scores_relu = []

for i in range(k):
  print('processing fold #', i)
  val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]
  partial_train_data = np.concatenate([x_train[:i * num_val_samples],x_train[(i + 1) * num_val_samples:]],  axis=0)
  # print(partial_train_data)
  partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)
  model = build_model()
  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores_relu.append(val_mae)

"""# **Train the final MOdel**"""

model = build_model()
model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=0)
test_mse_score, test_mae_score = model.evaluate(x_test, y_test)
print(test_mae_score)

"""# **Prediction Step**"""

len(model.predict(x_test))

model.predict(x_test)

